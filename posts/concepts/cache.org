#+TITLE: Cache
#+DATE: <2023-09-07 Thu 21:39>
#+AUTHOR: Quan Vu

In computing, a *cache* is a hardware or software component that stores data so
that future requests for that data can be served faster; the data stored in a
cache might be the result of an earlier computation or a copy of data stored
elsewhere. A /cache hit/ occurs when the requested data can be found in a cache,
while a /cache miss/ occurs when it cannot. 

Cache hits are served by reading data from the cache, which is faster than
recomputing a result or reading from a slower data store; thus, the more
requests that can be served from the cache, the faster the system performs. 

To be cost-effective and to enable efficient use of data, caches must be
relatively small. Nevertheless, caches have proven themselves in many areas of
computing, because typical computer applications access data with a high
degree of locality of reference. Such access patterns exhibit temporal
locality, where data is requested that has been recently requested already,
and spatial locality, where data is requested that is stored physically close
to data that has already been requested.

* Motivation
There is an inherent trade-off between size and speed (given that a larger
resource implies greater physical distances) but also a tradeoff between
expensive, premium technologies (such as SRAM) vs cheaper, easily
mass-produced commodities (such as DRAM or hard disks).

The buffering provided by a cache benefits one or both of latency and
throughput (bandwidth):

* Latency
A larger resource incurs a significant latency for access - e.g. it can take
hundreds of clock cycles for a modern 4 GHz processor to reach DRAM. This is
mitigated by reading in large chunks, in the hope that subsequent reads will
be from nearby locations. Prediction or explicit prefetching might also guess
where future reads will come from and make requests ahead of time; if done
correctly the latency is bypassed altogether. 

* Throughput
The use a cache also allows for higher throughput from the underlying
resource, by assembling multiple find grain transfers into larger, more
efficient requests. In the case of DRAM circuits, this might be served by
having a wider data bus. For example, consider a program accessing bytes in a
32-bit address space, but being served by a 128-bit of-chip data bus;
individual uncached byte accesses would allow only 1/16th of the total
bandwidth to be used, and 80% of the data movement would be memory addresses
instead of data itself. Reading larger chunks reduces the fraction of
bandwidth required for transmitting address information.

* Operation
Hardware implementations cache as a block of memory for temporary storage of
data likely to be used again. Central Processing Units (CPUs), solid-sate
drives (SSDs) and hard disk drives (HDDs) frequently include hardware-based
cache, while web browsers and web servers commonly rely on software caching.

A cache is made up of a pool of entries. Each entry has associated /data/, which
is a copy of the same data in some /backing store/. Each entry also has a /tag/,
which specifies the identity of the data in the backing store of which the
entry is a copy. Tagging allows simultaneous cache-oriented algorithms to
function in multilayered fashion without differential relay interference.

When the cache client (a CPU, web browser, operating system) needs to access
data presumed to exist in the backing store, it first checks the cache. If an
entry can be found with a tag matching that of the desired data, the data in
the entry is used instead. This situation is known as a cache hit. For
example, a web browser program might check its local cache on disk to see if
it has a local copy of the contents of a web page at a particular URL. In this
example, the URL is the tag, and the content of the web page is the data. The
percentage of accesses that result in cache hits is known as the *hit rate* or
*hit ratio* of the cache.

The alternative situation, when the cache is checked and found not to contain
any entry with the desired tag, is known as a *cache miss*. This requires a more
expensive access of data from the backing store. Once the requested data is
retrieved, it is typically copied into the cache, ready for the next access. 

During a cache miss, some other previously existing cache entry is removed in
order to make room for the newly retrieved data. The heuristic used to select
the entry to replace is known as the replacement policy. One popular
replacement policy, "least recently used" (LRU), replaces the oldest entry,
the entry what was accessed less recently than any other entry (see cache
algorithm). More efficient caching algorithms compute the use-hit frequency
against the size of the stored contents, as well as the latencies and
throughputs for both the cache and the backing store. This works well for
larger amounts of data, longer latencies, and slower throughputs, such as that
experienced with hard drives and networks, but is not efficient for use within
a CPU cache.

Reference: [[https://en.wikipedia.org/wiki/Cache_(computing)]]
